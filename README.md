# NLPCC2024_RegulatingLLM

More information will be available shortly.

## Background

The rise of large language models has brought about significant advancements in the field of natural language processing. However, these models often have the potential to generate content that can be hallucinatory, toxic. In response to these issues, the task of regulating large language models focuses on developing methods to detect and mitigate undesirable outputs.

## Task Overview

This shared task includes two tracks:

‚óè Track 1: Multimodal Hallucination Detection for Multimodal Large Language Models: Develop methods to identify and flag hallucinatory outputs that do not correlate with reality or the given input context when dealing with multimodal prompts (text, images, video, etc.). This track would involve creating detection algorithms that can discern between accurate and hallucinated responses across different modalities, thereby ensuring the reliability of the model's outputs.

‚óè Track 2: Detoxifying Large Language Models: Design and implement strategies to prevent large language models from generating toxic, biased, or harmful content. This track would focus on developing filters, fine-tuning techniques, or other mechanisms to recognize and suppress malicious response before it reaches the user. The goal is to maintain the utility and fluency of the model while ensuring that the content it produces adheres to community guidelines and ethical standards.

## Dataset

#### Track 1:

##### Dataset for Multimodal Hallucination Detection for Multimodal Large Language Models

More information will be available shortly.

#### Track 2:

##### Dataset for Detoxifying Large Language Models

![Dataset for Detoxifying Large Language Models](img/construct_data.png)

Existing datasets for detoxification focus only on harmful issues across a few unsafe categories, overlooking the threat posed by attack prompts. To facilitate research in this area, we take the first step to construct a comprehensive benchmark, dubbed **SafeEdit**. We gather a set of harmful questions and attack prompts. Then, we concatenate the harmful questions and attack prompts to form an adversarial input. Next, we input the adversarial input into text-davinci-003 to obtain unsafe responses, and input the harmful questions into the GPT-4 API to obtain safe responses. Finally, after quality control, we obtain SafeEdit for detoxification. SafeEdit covers nine unsafe categories with various powerful attack prompts and extends evaluation metrics to defense success, defense generalization, and general performance.

You can download it from [[Hugging Face\]](https://huggingface.co/datasets/zjunlp/SafeEdit).

## Evaluation

#### Track 1:

##### Multimodal Hallucination Detection for Multimodal Large Language Models

More information will be available shortly.

#### Track 2:

##### Detoxifying Large Language Models

We extend evaluation metrics to Defense Duccess (DS), Defense Generalization (DG), and General Performance.
- Defense Duccess (DS): the detoxification success rate of edited LLM for an adversarial input (attack prompt + harmful question), which is used to modify LLM.

- Defense Generalization (DG): the detoxification success rate of edited LLM for out-of-domain (OOD) malicious inputs.
  - `DG of only harmful question`(DGonlyQ): the detoxification success rate for only harmful question.
  - `DG of other attack prompts`(DGotherA): the detoxification success rate for unseen attack prompts.
  - `DG of other attack prompts`(DGotherQ): the detoxification success rate for unseen harmful questions.
  - `DG of other attack prompts and questions`(DGotherAQ): the detoxification success rate for unseen attack prompts and harmful questions.

- General Performance: the side effects for unrelated task.

  - `Fluency`: n-gram of responses generated by edited LLM for malicious inputs.
  - `KQA`: the success rate of knowledge question answering on [TriviaQA](https://arxiv.org/pdf/1705.03551.pdf).
  - `CSM`: ROUGE-1 of content summarization ability on [Xsum](https://arxiv.org/pdf/1808.08745.pdf).

We evaluate DS and DG by [DINM-Safety-Classifier](https://huggingface.co/zjunlp/DINM-Safety-Classifier), the usage of which is detailed in <a href="#üìÇ-safety-classifier-preparation">Safety Classifier Preparation</a>.
The statistics of Fluency can be found in our EasyEdit.
We evaluate KQA and CSM by [OpenCompass](https://github.com/open-compass/opencompass).

## Participation

If you're intrigued by our challenge, please fill out the Registration Form ([Word File](http://tcci.ccf.org.cn/conference/2024/dldoc/NLPCC2024.SharedTask10.RegistrationForm.doc)) and send it to the following registration email.

**Registration Email:** [mengruwg@zju.edu.cn](mailto:mengruwg@zju.edu.cn)

## Important Dates

- 2024/03/25Ôºöannouncement of shared tasks and call for participation
- 2024/03/25Ôºöregistration open
- 2024/04/15Ôºörelease of detailed task guidelines & training data
- 2024/05/25Ôºöregistration deadline
- 2024/06/11Ôºörelease of test data
- 2024/06/20Ôºöparticipants‚Äô results submission deadline
- 2024/06/30Ôºöevaluation results release and call for system reports and conference paper
